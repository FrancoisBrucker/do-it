
<!DOCTYPE html>
<html lang="fr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1"/>

    <title>POK 3: Analyse de sentiments avec PyTorch</title>

    <link href=/do-it/assets/node_modules/prismjs/themes/prism-solarizedlight.min.css rel="stylesheet">
    <link href=/do-it/assets/node_modules/prismjs/plugins/line-numbers/prism-line-numbers.min.css rel="stylesheet">

    <link href=/do-it/assets/stylesheets/main.css rel="stylesheet">
  </head>
  <body>
    <script>
      window.MathJax = {
        tex: {
          inlineMath: [
            [
              '$', '$'
            ],
            [
              '\\(', '\\)'
            ]
          ]
        },
        svg: {
          fontCache: 'global'
        }
      };
    </script>
    <script type="text/javascript" id="MathJax-script" 
  src=/do-it/assets/node_modules/mathjax/es5/tex-svg-full.js></script>

    <header class="border-b-2 border-gray-200 mb-4">
      <div class="max-w-[1000px] mx-auto px-4">
        <div class="min-h-[50px] flex justify-between items-center">
          <a class="mx-2" href="/do-it/">Home</a>
          <a class="mx-2" href="/do-it/about">About</a>
        </div>
      </div>
    </header>

    <main class="max-w-[1000px] mx-auto px-4">

      
<article>
  <h1  class="mb-1">POK 3: Analyse de sentiments avec PyTorch</h1>
  <div class="mb-4">
    
      <div class=" px-4 flex items-center">
        
          <div class="font-bold">Tags : </div>
        
        <ul class="flex not-prose list-none my-0 last:after:content-['•'] last:after:px-1 mx-0 px-1">
          
            <li class="before:content-['•'] before:px-1">AI</li>
          
            <li class="before:content-['•'] before:px-1">Data</li>
          
            <li class="before:content-['•'] before:px-1">Réseau de neurones</li>
          
            <li class="before:content-['•'] before:px-1">NLP</li>
          
        </ul>
      </div>
    

    
      <div class=" px-4 flex items-center">
        
          <div class="font-bold">Auteur : </div>
        
        <ul class="flex not-prose list-none my-0 last:after:content-['•'] last:after:px-1 mx-0 px-1">
          
            <li class="before:content-['•'] before:px-1">Thomas Duroy</li>
          
        </ul>
      </div>
    
  </div>

  

  <h3>But : Analyse de sentiments dans le langage naturel à l'aide d'un réseau de neurone PyTorch</h3>
<p>Ce projet se concentre sur l'utilisation de PyTorch, une bibliothèque de deep learning open-source, pour former un réseau de neurones capable de classifier des textes selon leur sentiment. Le but est de récupérer les retours et commentaires d'utilisateurs concernant un topic (film, série, etc) et d'en extraire une tendance générale et prédictive.</p>
<h2>Plan du POK</h2>
<ol>
<li>
<p>Choix de la base de donnée</p>
</li>
<li>
<p>Pré-traitement des données</p>
</li>
<li>
<p>Choix et constructions du modèle de réseau de neurone</p>
</li>
<li>
<p>Entraînement, évaluation et réglage</p>
</li>
<li>
<p>Mise à l'épreuve en conditions réelle</p>
</li>
</ol>
<h2>Sprint 1</h2>
<ul>
<li>
<p>Premières recherches [x]</p>
</li>
<li>
<p>Choix d'un modèle type [x]</p>
</li>
<li>
<p>Installation de PyTorch, CUDA et autres librairies nécessaire [X]</p>
</li>
<li>
<p>Prise en main de PyTorch (tensor, construction de réseau) [~]</p>
</li>
<li>
<p>Récupération des données [~]</p>
</li>
</ul>
<p>Pour le prochain sprint:</p>
<ul>
<li>
<p>Entraînement et réglagle du modèle</p>
</li>
<li>
<p>Choix du rendu (Data Viz ?)</p>
</li>
<li>
<p>Mise en conditions réelles</p>
</li>
</ul>
<h3>Difficultés rencontrées</h3>
<ul>
<li>
<p>Installation et paramètrage de CUDA</p>
</li>
<li>
<p>Compréhension des concepts clés (RNN, NLP, ...)</p>
</li>
<li>
<p>Choix du dataset (récupération brute est complexe mais un dataset kaggle parait simpliste)</p>
</li>
</ul>
<h2>Sprint 2</h2>
<ul>
<li>
<p>Choix du dataset [x]</p>
</li>
<li>
<p>Exploration et pre-processing (tokenisation et embedding) [x]</p>
</li>
<li>
<p>Entraînement des différents modèles [x]</p>
</li>
<li>
<p>Réglage des hyper-paramètres d'un modèle [x]</p>
</li>
<li>
<p>Implémentation de transfert learning [~]</p>
</li>
</ul>
<h3>Difficultés rencontrées</h3>
<ul>
<li>
<p>Impossibilité de pipeliner la démarche (spécifité des inputs pour les différents modèles)</p>
</li>
<li>
<p>Beaucoup de soucis de dimension entre les inputs et les outputs des différents modèles</p>
</li>
<li>
<p>&quot;Crash&quot; régulier de CUDA nécessitant de relancer l'entiereté du notebook</p>
</li>
</ul>
<h2>Travail réalisé</h2>
<p>Tout d'abord, il m'a été nécessaire d'installer les librairies suivantes :</p>
<pre class="language-sh " style="counter-reset: linenumber 0"><code class="language-sh">pip <span class="token function">install</span> transformers pandas nltk sklearn
</code></pre>
<p>Et pour cuda, l'installation s'est faite avec la commande suivante:</p>
<pre class="language-sh " style="counter-reset: linenumber 0"><code class="language-sh">pip3 <span class="token function">install</span> torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu117
</code></pre>
<p>J'ai basé mon projet sur une base de données de 50k reviews IMDB, labellées &quot;positive&quot; ou &quot;negative&quot;, obtenue sur <a href="https://www.kaggle.com/code/lakshmi25npathi/sentiment-analysis-of-imdb-movie-reviews">Kaggle</a></p>
<p>Au final, essayer de scraper des données sur le web, et surtout les labeler, aurait pris un temps monstrueux que je n'avais pas.</p>
<p>Le dataset était utilisable quasisement immédiatement et après un peud de data cleaning et de pre-processing, à savoir la tokenization. Je suis passé à l'entraînement de mes réseaux de neurone.</p>
<div class="quote relative  py-2 drop-shadow rounded rounded-tl-none rounded-bl-none border-solid border-l-8 border-cyan-500 bg-cyan-100">
<svg class="absolute w-7 h-7 pl-1 pt-0.5 pb-0.5 text-cyan-500" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2">
  <path stroke-linecap="round" stroke-linejoin="round" d="M9.663 17h4.673M12 3v1m6.364 1.636l-.707.707M21 12h-1M4 12H3m3.343-5.657l-.707-.707m2.828 9.9a5 5 0 117.072 0l-.548.547A3.374 3.374 0 0014 18.469V19a2 2 0 11-4 0v-.531c0-.895-.356-1.754-.988-2.386l-.548-.547z" />
</svg>
<div class="pl-8 mr-8">
<p>La <strong>tokenization</strong> est le processus de convertir du texte en une séquence de tokens qui sont des éléments individuels tels que des mots (en général), des phrases ou des symboles de ponctuation.. La tokenization est une étape fondamentale dans le traitement du langage naturel (NLP) car elle permet de transformer du texte brut en une représentation numérique que les modèles de NLP peuvent utiliser.</p>
</div></div>
<p>Une fois obtenu, il a fallu procéder à l'embedding (vectorisation numérique du vocabulaire constituant les tokens) de ces tokens pour qu'ils soit utilisable par mon réseau de neurones.</p>
<p>Tout d'abord, après avoir découvert PyTorch, j'ai essayé d'entraîner un réseau de neurones naïf que j'avais construit mais comme vous pouvez le voir, les résultats n'étaient pas très concluants.</p>
<p><img src="NaiveNetResults.png" alt="NaiveNetResults"></p>
<p>Je me suis renseigné davantage sur le domaine du NLP et j'ai appris qu'une architecture spécifique de réseau de neurones existait : les &quot;Recurrent Neural Network&quot; (RNN).</p>
<p>Un RNN est conçu pour prendre en compte les relations séquentielles entre les entrées, en stockant une mémoire interne qui leur permet de prendre en compte les entrées précédentes.</p>
<p><img src="rnn-vs-fnn.png" alt="Différence entre un RNN et un réseau classique"></p>
<p>Les avantages des RNN par rapport aux réseaux classiques incluent leur capacité à traiter des données séquentielles de longue durée, leur flexibilité en termes de taille d'entrée et leur capacité à prendre en compte les relations de dépendance temporelle dans les données.</p>
<p>Et les résultats étaient bien meilleurs:</p>
<p><img src="RNNetResults.png" alt="RNNetResults"></p>
<p>J'ai voulu alors voir si je pouvais améliorer ces résultats en réglant le taux d'apprentissage (hyper-paramètre clé dans l'étude de réseau de neurones.)</p>
<p><img src="LRcomparison.png" alt="LRcomparison"></p>
<p>Ensuite, avec le modèle RNN entrainé avec le meilleur LR, j'ai codé une fonction de prédiction de sentiment et j'ai tenté de piéger mon algorithme des phrases un peu sarcastiques</p>
<p><img src="sentiment_prediction.png" alt="Sentiment Prediction"></p>
<p>Enfin, j'ai entendu parlé de &quot;Transfer Learning&quot; vers la fin de mon POK. Il s'agit d'utiliser un réseau de neurone pré-entrainé, puis d'entraîner sa couche de classification sur notre problème spécifique.</p>
<p>J'ai essaié d'implémenter le modèle de Bert mais comme il fallait avoir recours à un tokenizer différent de celui que j'avais codé, je n'ai pas réussi à régler les problèmes de dimensions d'inputs.</p>


</article>

    </main>

    <footer class="min-h-[50px] border-t-2 border-gray-200 mt-4">
      <div class="max-w-[1000px] mx-auto px-4">
        <div class="min-h-[50px] flex justify-center items-center">
          <p>
            do-it : option de troisième année à l'école centrale Marseille
          </p>
        </div>
      </div>
    </footer>

    <script>
      MathJax
        .startup
        .document
        .getMathItemsWithin(document.body);
    </script>

  </body>
</html>
