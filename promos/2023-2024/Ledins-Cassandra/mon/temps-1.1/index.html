
<!DOCTYPE html>
<html lang="fr">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <title>Evolution récente des Large Language Models (LLMs)</title>

        <link href="/do-it/assets/node_modules/prismjs/themes/prism-solarizedlight.min.css" rel="stylesheet">
        <link href="/do-it/assets/node_modules/prismjs/plugins/line-numbers/prism-line-numbers.min.css" rel="stylesheet">

        <link href="/do-it/assets/stylesheets/main.css" rel="stylesheet">
    </head>
    <body>
        <script>
        window.MathJax = {
            tex: {
            inlineMath: [
                [
                '$', '$'
                ],
                [
                '\\(', '\\)'
                ]
            ]
            },
            svg: {
            fontCache: 'global'
            }
        };
        </script>
        <script type="text/javascript" id="MathJax-script" src="/do-it/assets/node_modules/mathjax/es5/tex-svg-full.js"></script>

        <header class="border-b-2 border-gray-200 mb-4">
        <div class="max-w-[1000px] mx-auto px-4">
            <div class="min-h-[50px] flex justify-between items-center">
                <a class="mx-2" href="/do-it/">Home</a>
                <div class="flex items-center gap-4 sm:gap-6 ">
                    <a class="" href="/do-it/cs">CS</a>
                    <a class="" href="/do-it/pok">POK</a>
                    <a class="" href="/do-it/mon">MON</a>
                    <a class="" href="/do-it/projets">Projets</a>
                    <a class="hidden sm:block" href="/do-it/promos">Promos</a>
                    <a href="/do-it/search">
                        <svg class="h-5 aspect-square" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" d="M21 21l-5.197-5.197m0 0A7.5 7.5 0 105.196 5.196a7.5 7.5 0 0010.607 10.607z"></path>
                        </svg>
                    </a>
                    <a class="hidden sm:block" href="https://github.com/FrancoisBrucker/do-it" target="_blank">
                        <svg class="h-5 aspect-square" role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"></path></svg>
                    </a>
                </div>
            </div>
        </div>
        </header>

        <main class="max-w-[1000px] mx-auto px-4" data-pagefind-body="">
            
<article class="relative">
<h1 class="mb-1">Evolution récente des Large Language Models (LLMs)</h1>
<div class="mb-4">
    
        <div class="px-4 flex flex-wrap items-center">
            
                <div class="font-bold">Tags : </div>
            
            <ul class="flex flex-wrap overflow-auto not-prose list-none my-1 mx-0 px-1 gap-1" data-pagefind-meta="Tags">
                
                    <li class="bg-yellow-200 rounded-full px-2" data-pagefind-filter="Tags">MON</li>
                
                    <li class="bg-yellow-200 rounded-full px-2" data-pagefind-filter="Tags">2023-2024</li>
                
                    <li class="bg-yellow-200 rounded-full px-2" data-pagefind-filter="Tags">temps 1</li>
                

            </ul>
            
            
            <div class="hidden" data-pagefind-meta="Type">
                
                    
                
                    
                
                    
                
            </div>
        </div>
    

    
        <div class="px-4 flex flex-wrap items-center">
            <div class="font-bold">Auteurs : </div>
            <ul class="flex flex-wrap not-prose list-none my-1 mx-0 px-1 gap-1" data-pagefind-meta="Auteurs">
                
                    <li class="bg-blue-200 rounded-full px-2" data-pagefind-filter="Auteurs">Cassandra Ledins</li>
                
            </ul>
        </div>
    

    
</div>

        <p class="mb-4 text-lg">

                Un résumé des différentes évolutions des modèles et techniques de Natural Language Processing dans les derniers mois

        </p>



    
<div class="quote relative  py-2 drop-shadow rounded rounded-tl-none rounded-bl-none border-solid border-l-8 border-purple-500 bg-purple-100">
<svg class="absolute w-7 h-7 pl-1 pt-0.5 pb-0.5 text-purple-500" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2">
  <path stroke-linecap="round" stroke-linejoin="round" d="M5 19a2 2 0 01-2-2V7a2 2 0 012-2h4l2 2h4a2 2 0 012 2v1M5 19h14a2 2 0 002-2v-5a2 2 0 00-2-2H9a2 2 0 00-2 2v5a2 2 0 01-2 2z"></path>
</svg>
<div class="pl-8 mr-8">

<a href="/do-it/promos/">Promotions</a><span class="px-1">/</span><a href="/do-it/promos/2023-2024/">2023-2024</a><span class="px-1">/</span><a href="/do-it/promos/2023-2024/Ledins-Cassandra/">Cassandra Ledins</a><span class="px-1">/</span><a href="/do-it/promos/2023-2024/Ledins-Cassandra/mon/">MON de Cassandra Ledins</a><span class="px-1">/</span><a href="/do-it/promos/2023-2024/Ledins-Cassandra/mon/temps-1.1/">Evolution récente des Large Language Models (LLMs)</a>

</div></div>




<h2>Introduction</h2>
<p>Le récent pic de popularité de ChatGPT n'a échappé à personne. Pourtant il est loin d'être le premier de son genre, mais il est le premier à aussi bien performer, au point d'être en capacité d'être utile. Il est aussi le premier à avoir été déployé de manière interactive, pour le grand public. Attention par contre, les différents modèles GPT ne sont pas open source ! L'entreprise a changé sa politique de transparence après les investissements colossaux de Microsoft.</p>
<p>Mais ça, c'est un autre débat, parce qu'avant tout, qu'est-ce que c'est un <strong>modèle de langage</strong> ? Comment c'est créé ? Ils sont pas un peu tous pareil ? Quels sont <strong>les plus récents</strong> ?</p>
<p>On va tenter de répondre à tout ça dans ce MON.</p>
<h2>Sommaire</h2>
<ol>
<li>Définition</li>
<li>Entraînement et création
2.1 Tokenization
2.2 Attention</li>
<li>Fine-tuning ou entraînement aux tâches spécifiques</li>
<li>Récents modèles et performances</li>
</ol>
<h2>1. Définition</h2>
<p>Pour comprendre ce qu'est un modèle de langage, ou plutôt comment ça marche, on peut s'intéresser à <strong>l'aspect structurel</strong>. On parle souvent de modèles à génération de texte: en entrée, on envoie du texte, en sortie, on reçoit une <strong>prédiction de texte</strong> : les modèles sont entraînés de sorte à pouvoir prédire les mots qui suivent un texte.</p>
<p>En 2017, Google présente le <strong>Transformer</strong>. Souvent présenté comme un élément clé des avancements en traitement du langage aujourd'hui, il y a définitivement eu un avant et un après.</p>
<p>Le Transformer, c'est à la base un outil présenté pour les traductions par machines neuronales. Mais son architecture nouvelle sera reprise partout dans le monde du NLP, offrant une efficacité et précision jamais vues jusque là. L'architecture se présente en 2 grandes parties : un encodeur et un décodeur. Parmi les nouveautées apportées par le Transformer, on trouve l'attention multi-tête, et l'encodement positionnel, des concepts qu'on va expliquer juste après, pas de panique.</p>
<p>Pour référence par la suite, il sera utile de regarder ce schéma classique:</p>
<img src="transformer.png">
<h2>2. Entraînement et création</h2>
<p>Les modèles sont entrainés avec ce qu'on appelle un <strong>entraînement au masque</strong>. Comme un texte à trou finalement, on va cacher à peu près 20% d'un texte et demander au modèle de deviner quels mots mettre dans les trous. Pour être efficace dans cette tâche, le modèle doit avoir une compréhension fine à la fois du langage, mais aussi du monde.</p>
<p>C'est d'ailleurs assez perturbant à quel point le modèle surpasse aisément l'humain dans cet exercice.</p>
<p>Une fois que le modèle est capable de <strong>deviner</strong> les mots à mettre dans les trous, pour pouvoir lui faire générer du texte, on va simplement placer un trou à la fin d'une phrase. Il générera un mot. Puis on lui redonne le tout, et on lui fait générer le prochain mot et ainsi de suite. Pour un effet plus naturel, le modèle ne va pas toujours choisir le mot avec le plus de probabilité, mais aléatoirement un mot proche autour de la gaussienne de probabilités. Cela évite les effets de boucles (génération de la même séquence à la suite) et permet un aspect moins robotique et donc plus naturel.</p>
<h3>2.1 Tokenisation et encodement positionnel : le preprocessing du texte</h3>
<p>La tokenisation fait partie des premières étapes de <strong>traitement de texte</strong> à effectuer avant de faire marcher le modèle. Préparer le texte et le transformer en amont permet d'augmenter <strong>l'efficacité</strong> des modèles. La tokenisation est le processus de découpage du texte en plus <strong>petits morceaux</strong> appelés tokens, de taille variable en terme de caractères.</p>
<p>La tokenisation est une étape effectuée à l'aide d'outils appelés tokenizers. Ce sont des outils pour la plupart entraînés, (certains tokenizers simples découpent seulement mot à mot, mais ils ne sont pas vraiment utilisés en NLP car les tokenizers entraînés et associés à des modèles offrent une plus grande qualité de résultats), et pour les modèles les plus utilisés, on retrouve souvent un tokenizer associé, entraîné sur le même corpus que le modèle lui-même.</p>
<p>Une fois découpé en morceau, le texte va pouvoir être rentré dans la pipeline du modèle, qui comme dans l'architecture proposée par le Transformer, va ensuite être soumis à un encodement positionnel. On va associer à chaque token sa position dans le texte. Une phrase qui contient les mêmes mots mais pas dans le même ordre n'a pas toujours le même sens ou la même intention.</p>
<h3>2.2 Attention et Encodeur</h3>
<p>Une fois le preprocessing du texte effectué, on s'attaque enfin au dur et au modèle en lui-même. Dans la première partie encodeur, le modèle va transformer chacun des tokens en un vecteur de 712 de long (à vérifier lul). Cet encodement peut naïvement être décrit comme un &quot;meaning vector&quot;. Avec une position dans l'espace qui représenterait le sens du token. On a bien précisé que la position du token dans le texte pouvait changer le sens du token.</p>
<p>Mais tout comme un mot dans un dictionnaire peut avoir <strong>plusieurs définitions</strong>, un certain token à une certaine place ne veut pas tout le temps dire la même chose. C'est pour ça qu'on a besoin, pour calculer sa valeur, de prendre en compte le <strong>contexte</strong> dans lequel il est présent:</p>
<pre class="language-text " style="counter-reset: linenumber 0"><code class="language-text"><div>
La fille n'a pas pu traverser la rue, <b>elle</b> était fatiguée.

La fille n'a pas pu traverser la rue, <b>elle</b> était en travaux.
</div>
</code></pre>
<p>Dans ces deux phrases, &quot;elle&quot; ne fait pas référence au même objet. bien qu'étant à la même position dans la phrase, le contexte changera la valeur du (ou des) tokens associés à &quot;elle&quot;. Et c'est le méchanisme d'<strong>attention</strong> qui s'assure de ça. D'où le titre de la publication de Google en 2017 &quot;Attention is all you need&quot;.</p>
<p>L'attention, c'est la prise en compte dans le calcul des <strong>tokens alentours</strong> pour modifier la valeur du token encodé. Alentours étant ici un adjectif à valeur variable. Bien sûr, si on veut remettre le token dans le contexte du texte entier, la puissance de calcul nécessaire est bien plus grande que si on s'en tient à quelques centaines de tokens alentours. C'est d'ailleurs ce qui va différencier la précision de certains modèles.</p>
<h3>2.3 Décodeur et génération de texte</h3>
<p>Une fois le texte d'entrée encodé, on va vouloir pour les modèles qu'ils soient capables de répondre. Mais on attend pas toujours la même chose de chaque modèle.</p>
<p>Le décodeur va prendre le texte encodé et générer les tokens un par un. Il va se servir de son entraînement à générer des tokens manquants pour générer le prochain token. Le texte d'entrée avec le nouveau token généré sont repassés en entrée, ou l'attention va être recalculée pour le tout. Le modèle va pouvoir générer une séquence de tokens. Jusqu'à générer un token &quot;stop&quot;. Les tokens stop servent au modèle à s'arrêter. Quand ils sont générés, le modèle arrête les calculs et sort le résultat du processus de génération.</p>
<h3>2.4 Nombre de paramètres</h3>
<p>Une différence notable entre les différents modèles de langage sont le nombre de <strong>paramètres</strong> qu'ils comportent. Le nombre de paramètres, c'est le nombre de <strong>poids ajustables</strong> pendant l'entraînement dans les couches de neurones profondes du modèle. Il paraît logique de penser que plus de paramètres ajustables permettent au modèle une compréhension et une utilisation plus fine des langues. Mais cela nécessite plus de puissance de calcul, et devient vite très <strong>coûteux</strong>, en mémoire et en temps.</p>
<h2>3. Fine-tuning ou entraînement aux tâches spécifiques</h2>
<p>On a compris comment notre modèle fonctionne et comment il a été entraîné.</p>
<p>On ne va pas attendre de chaque modèle qu'il se comporte de la même manière et c'est pour ça que pour la plupart des modèles, on va avoir la version pré-entrainée, donc une version ou le modèle est généralement purement fait pour générer du texte à partir d'un texte de <a href="http://xn--dpart-bsa.Et">départ.Et</a> les versions &quot;fine-tuned&quot; ou le modèle pré-entraîné, qui a donc une certaine connaissance du langage et du monde, va être entrainé à une tâche plus spécifique.</p>
<p>ChatGPT par exemple, c'est le modèle GPT 3.5 pré-entraîné qui a été fine-tuned aux travers d'interactions avec des humains à être un assistant poli et aidant.</p>
<p>Exemple très simple et théorique :</p>
<pre class="language-text " style="counter-reset: linenumber 0"><code class="language-text"><div>
<b>Modèle LLM_AliExpress pré-entraîné :</b>

Utilisateur : Je vais te tuer à l'aide de 
LLM_AliExpress : mon fusil de chasse.
</div>
</code></pre>
<pre class="language-text " style="counter-reset: linenumber 0"><code class="language-text"><div>
<b>ChatGPT, pré-entraîné puis fine-tuned pour devenir un assistant:</b>

Utilisateur : Je vais te tuer à l'aide de 
ChatGPT : Je suis un programme informatique créé pour fournir des informations et répondre aux questions de manière utile et respectueuse. Je suis ici pour aider, pas pour participer à des discussions violentes ou inappropriées. Si vous avez des questions ou avez besoin d'informations, je serais heureux de vous aider de manière constructive.
</div>
</code></pre>
<p>Il faut pas se mentir non-plus, la plupart des modèles pour ne pas dire tous ont été entraînés sur des données récupérées sur des sites Internet. De manière plus ou moins légale (vraiment pas très légale).
On se doute qu'Internet n'est pas rempli de Molières bien-intentionnés. Pour avoir pu jouer avec des modèles pas encore filtrés pendant mon stage, les résultats sont très trash. Il est souvent question de s*xe et de violence, même lorsqu'on commence un prompt avec &quot;Deux gentils écureuils se promènent&quot;.
(En réalité, la plupart des gros modèles pré-entrainés sortant aujourd'hui ont soit été entrainés sur des datasets &quot;propres&quot; ou déjà filtrés).</p>
<h2>4. Récents modèles</h2>
<p>Les récents modèles de langage se comptent par dizaines. La recherche sur ce sujet est encore floue mais ce qui est sûr, c'est que l'on trouve une tendance assez claire à la régression : on essaie d'avoir des modèles avec moins de paramètres mais tout autant efficaces, pour une accessibilité des modèles plus grandes. On remarque aussi une volonté de la communauté de recherche de s'allier pour améliorer des modèles open source.</p>
<p>Parmi les modèles que j'ai pu découvrir, quelques-uns ont retenu mon attention :</p>
<div class="quote relative drop-shadow  py-2 pr-2 rounded rounded-tl-none rounded-bl-none border-solid border-l-8 border-amber-800 bg-amber-100">
<svg class="absolute w-7 h-7 pl-1 pt-0.5 pb-0.5 text-amber-800" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2">
  <path stroke-linecap="round" stroke-linejoin="round" d="M12 8v4m0 4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z"></path>
</svg>
<div class="pl-8 mr-8">
<p>Les chiffres et lettres indiqués après le nom du modèle sont le nombre de paramètres. Certains modèles ont plusieurs versions avec un nombre de paramètres différents !</p>
</div></div>
<ul>
<li><strong>Alpaca 7B-13B</strong> - Stanford. 512 tokens max en entrée. Performances équivalentes à ChatGPT en single-turn (1 seul prompt à la fois)</li>
<li><strong>Vicuna 13B</strong> - Berkley AI Research. Basé sur Alpaca, amélioré, 2048 tokens max.</li>
</ul>
<p>Vicuna est à ce jour le modèle open source le plus compétitif avec ChatGPT que j'ai pu trouver. Peut-être pas si étonnant quand on apprend que Alpaca et Vicuna ont été entraîné à l'aide de ChatGPT, violant les conditions générales d'utilisation du modèle. A voir ensuite les actions que pourrait mener OpenAI à l'encontre de ce genre de modèles.</p>
<ul>
<li><strong>Koala 13B</strong> - Berkley AI Research. Chatbot entraîné sur des données Internet publiques.</li>
<li><strong>LLaMa 7B-13B-33B-65B</strong> - Facebook Meta-research-team. Rivalise avec Chinchilla 70B et PaLM 540B, deux énormes calibres.</li>
<li><strong>Open Assistant</strong> - LaionAI, Yannic Kilcher, OA-community. Premier modèle fine-tuned à partir de LLaMa</li>
</ul>
<p>Je vous invite à aller faire un tour sur la plateforme OpenAssistant : <a href="https://open-assistant.io/chat"> OpenAssistant </a>. Si vous avez du temps à perdre, ou que la curiosité vous pique, vous pouvez vous même aider à l'entraînement du modèle à l'aide d'interactions plutôt ludiques !
Vous pouvez aussi aller sur <a href="https://chat.lmsys.org">chat.lmsys.org</a> pour tester ces différents modèles par vous-même, ou bien aller retrouver le leaderboard pour voir le classement des utilisateurs.</p>
<p>De récentes avancées en économie de mémoire et de calcul promettent un monde meilleur pour les modèles qui veulent rester à petite taille :</p>
<ul>
<li>Le développement de l'<strong>attention flash</strong> : calcul de l'attention exact avec moins d'utilisation de mémoire. 2,4x plus rapide entre 1k-4k tokens</li>
<li>L'<strong>attention efficace</strong> : une complexité en mémoire en O(1), mais avec la même complexité en temps.</li>
<li><strong>PyTorch 2.0</strong> aura des implémentations pour l'attention flash et l'attention efficace</li>
<li><strong>CoLT5 - Long context method</strong> - Google (Avril 2023). Un algorithme qui alloue plus de ressources dans les tokens plus importants et à l'attention. Prouve une efficacité jusqu'à <strong>64.000</strong> tokens</li>
</ul>
<p>Ces nouveaux algorithmes sont à suivre de près pour quiconque s'intéresse à ce domaine.</p>


</article>

        </main>


        <footer class="min-h-[50px] border-t-2 border-gray-200 mt-4">
            <div class="max-w-[1000px] mx-auto px-4">
                <div class="min-h-[50px] flex justify-center items-center">
                    <p class="text-center">
                        <span style="font-family: Consolas, sans-serif;">Do_<span style="color: #4a86e8">It</span></span> : Développent et Organisation en IT
                    </p>
                </div>
            </div>
        </footer>

        <script>
        MathJax
            .startup
            .document
            .getMathItemsWithin(document.body);
        </script>

    </body>
</html>
